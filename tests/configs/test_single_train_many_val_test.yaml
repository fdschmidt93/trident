# @package _global_

defaults:
  - override hydra/job_logging: disabled

work_dir: ${hydra:runtime.cwd}
data_dir: ${work_dir}/data/
print_config: False

seed: 42
callbacks: null

config_callbacks:
  datamodule.datasets:
    # for this config
    # do config callback as specified below
    _target_: trident.utils.hydra.expand
    merge_keys: ["train", "val", "test"]
    gen_keys: false

  datamodule.dataloaders:
    _target_: trident.utils.hydra.expand
    merge_keys: ["train", "val", "test"]
    gen_keys: true

  module.evaluation.prepare:
    _target_: trident.utils.hydra.expand
    merge_keys: ["val", "test"]
    gen_keys: true

  module.evaluation.step_outputs:
    _target_: trident.utils.hydra.expand
    merge_keys: ["val", "test"]
    gen_keys: true

  module.evaluation.metrics:
    _target_: trident.utils.hydra.expand
    merge_keys: ["val", "test"]
    gen_keys: true


trainer:
  _target_: lightning.Trainer
  max_epochs: 10
  accelerator: cpu
  enable_checkpointing: false

optimized_metric: "off_by_one/val/mse_loss"

module:
  _target_: tests.helpers.modules.ToyModule
  _recursive_: false
  optimizer:
    _target_: torch.optim.SGD
    lr: 5
  scheduler: null
  model:
    _target_: tests.helpers.modules.get_module

  evaluation:
    prepare:
      batch: null  
      outputs: null
      step_outputs: null 
    step_outputs:
      outputs:
        - preds
      batch:
        - labels
    metrics:
      mse_loss:
        metric:
          _partial_: true
          _target_: torch.nn.functional.mse_loss
        compute_on: "epoch_end"
        kwargs: 
          input: "outputs:preds"
          target: "outputs:labels"

datamodule:
  _target_: trident.TridentDataModule
  _recursive_: false
  datasets:
    _target_: tests.helpers.data.IdentityDataset
    _recursive_: false
    train:
      X:
        # a bit convoluted to cast arange to float in hydra config
        _target_: torch.eye
        _args_:
          - 10
      y:
        # a bit convoluted to cast arange to float in hydra config
        _target_: torch.Tensor.float
        _args_:
          - _target_: torch.arange
            start: 1
            end: 11
    val:
      _datasets_:
        off_by_one:
          X:
            _target_: tests.helpers.data.get_val_data
            to_: 5
          y:
            # a bit convoluted to cast arange to float in hydra config
            _target_: torch.Tensor.float
            _args_:
              - _target_: torch.arange
                end: 5
        off_by_two:
          X:
            _target_: tests.helpers.data.get_val_data
            from_: 5
          y:
            # a bit convoluted to cast arange to float in hydra config
            _target_: torch.Tensor.float
            _args_:
              - _target_: torch.arange
                start: 8
                end: 13
    test:
      _datasets_:
        off_by_one:
          X:
            _target_: tests.helpers.data.get_val_data
            to_: 5
          y:
            # a bit convoluted to cast arange to float in hydra config
            _target_: torch.Tensor.float
            _args_:
              - _target_: torch.arange
                end: 5
        off_by_two:
          X:
            _target_: tests.helpers.data.get_val_data
            from_: 5
          y:
            # a bit convoluted to cast arange to float in hydra config
            _target_: torch.Tensor.float
            _args_:
              - _target_: torch.arange
                start: 8
                end: 13
  dataloaders:
    _target_: torch.utils.data.dataloader.DataLoader
    collate_fn:
      _target_: tests.helpers.data.collate_fn
      _partial_: true
    batch_size: 10
    num_workers: 0
    pin_memory: true
    # the individual splits inherit the content of the dataset_cfg-level namespace
    # to avoid repeating configuration
    train:
      shuffle: false
    val:
      shuffle: false
    test:
      shuffle: false
