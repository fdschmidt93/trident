.. _readme:

.. |project| replace:: trident
.. _project: https://www.github.com/fdschmidt93/trident/

Using |project|
****************************

|project| is generic framework to train and evaluate models and represents a thin layer of convenience on top of

* `Pytorch <https://https://pytorch.org/>`_
* `Pytorch-Lightning <https://pytorch-lightning.readthedocs.io/>`_
* `Hydra <https://hydra.cc>`_
* `datasets <https://huggingface.co/docs/datasets/>`_
* `transformers <https://huggingface.co/transformers/>`_
* `torchmetrics <https://torchmetrics.readthedocs.io/>`_

It is quintessential that you familiarize yourself with Hydra as it constitutes the backbone for composing experiments in |project|.

Quick Start
===========

.. include:: installation.rst

Paradigms
---------

* **Simplicity:** follow existing patterns of our stack for easy understanding and extending
* **Low code:** trident is a wrapper first -- prefer implementations from well-maintained frameworks
* **Out-of-the-box:** provide readily available pipelines for popular benchmark datasets

Usage
-----

Typical usage of trident follows the below schema:

1. Clone the repo
2. Write a configuration for your model (see also :ref:`customization`)
3. Train on an existing experiment with `python run.py experiment=mnli module=my_model`

You can find existing pipelines at :repo:`experiments configs <configs/experiments/>`. A full experiment (incl. `module`) is defined in the :repo:`MNLI-TinyBert config <configs/experiment/mnli_tinybert.yaml>`.

Project Structure
-----------------

**Important Concepts**:

* `module` encapsulates a :obj:`pytorch_lightning.LightningModule` in a Hydra config (example: :repo:`sequence classification <configs/module/sequence_classification.yaml>`)
* `datamodule` represents a :obj:`pytorch_lightning.LightningDataModule` in a Hydra config (example: :repo:`MNLI <configs/datamodule/mnli.yaml>`)

.. code-block::
    
    ├── configs                 <- Hydra configuration files
    │   ├── callbacks               <- Callbacks for model training
    │   ├── datamodule              <- Datamodule configs
    │   ├── experiment              <- Experiment configs encapsulate **at least** everything except model (but may include models)
    │   ├── hparams_search          <- Hyperparameter search configs
    │   ├── hydra                   <- Hydra related configs
    │   ├── logger                  <- Logger configs
    │   ├── module                  <- Module configs: model (mixins & overrides), optimizer, scheduler
    │   ├── trainer                 <- Trainer configs
    │   │
    │   └── config.yaml             <- Main project configuration file
    │
    ├── logs                    <- Logs generated by Hydra and PyTorch Lightning loggers
    │
    ├── tests                   <- Tests of any kind
    │   ├── helpers                 <- A couple of testing utilities
    │   ├── shell                   <- Shell/command based tests
    │   └── unit                    <- Unit tests
    │
    ├── src
    │   ├── callbacks               
    │   ├── datamodules             
    │   ├── modules                 
    │   │  ├──── base               <- Definition of TridentModule
    │   │  ├──── mixins             <- Extend base module with methods
    │   │  ├──── functional         <- Extend base module functionally
    │   ├── utils                   
    │   │
    │   └── train.py                <- Training pipeline


(Trident)Module
---------------

Most importantly, |project| allows you to:

- Use your own LightningModules
- Embed your own model into the TridentModule
- Easily leverage existing frameworks like `transformers <https://huggingface.co/transformers/>`_ (see below)

A `TridentModule` is analogous to what a :obj:`pytorch_lightning.LightningDataModule` comprises (model, optimizer, scheduler, ...) and commonly defined with **shared and individual flags**. Here's an example for sequence classification.

First, the included model configurations inherit **shared options** from the :repo:`base module config <configs/module/base.yaml>`.

.. code-block:: yaml

    _recursive_: false
    _target_: src.modules.base.TridentModule
    defaults:
    - /optimizer: ${optimizer}  # global default (adamw: /configs/optimizer/adamw.yaml
    - /scheduler: ${scheduler}  # global default (linear warmup: /configs/scheduler/linear_warm_up.yaml)

`/configs/module/sequence_classification.yaml`

For instance, you can then 

.. code-block:: bash

    # change the learning rate
    python run.py experiment=mnli optimizer.lr=0.0001
    # set a different optimizer
    python run.py experiment=mnli optimizer=adam  # see /configs/optimizer/
    # no lr scheduler
    python run.py experiment=mnli scheduler=null



.. code-block:: yaml

    defaults:
    - base # inherit config from /configs/module/base.yaml
    - /evaluation: classification # 
    # optionally
    - /overrides: ...
    - /mixins: ...
    
    # you can set instructions here on how to embed your own model
    model:
      _target_: transformers.AutoModelForSequenceClassification.from_pretrained
      num_labels: ???
      pretrained_model_name_or_path: ???

Should you embed your own model, a TridentModule requires the model to `forward` be analogous to Huggingface transformers models, see the :py:meth:`src.modules.base.TridentModule.training_step`. Your model output should contain all required attributes for evaluation.

The `TridentModule` by default incorporates mixins for optimizer configuration and custom evaluation loops for the respective tasks (see :ref:`evaluation`).

DataModule
----------

The :obj:`src.datamodule.base.BaseDataModule` is initalized as follows.

.. code-block:: python
    
    # the python code is referenced explicitly here as opposed to yaml
    # to highlight optional flags
    def __init__(
        self,
        collate_fn: DictConfig, # collate_fn for dataloader
        batch_size: int = 8,
        num_workers: int = 8,
        pin_memory: bool = True,
        overrides: Optional[DictConfig] = None,
        # linked by default against collate_fn
        train_collate_fn: Optional[DictConfig] = None,  # for train_dataloader
        val_collate_fn: Optional[DictConfig] = None,    # for val_dataloader
        test_collate_fn: Optional[DictConfig] = None,   # for test_dataloader
        seed: int = 42,
    ):

All datamodules inherit from the base datamodule and are required to at least define a :obj:`setup` method. See the :repo:`config <configs/datamodule/mnli.yaml>` and corresponding :repo:`implementation <src/datamodules/mnli.py>` on how to define your own datamodules easily. 

In addition, overrides allows you to functionally override and extend the datamodule instance. See more at :ref:`function-override`.

At last, you can arbitrarily combine datamodules with the :obj:`src.datamodules.meta.MetaDatamodule` (example experiment: :repo:`config <configs/experiment/pawsx.yaml>`, TODO).


Extensibility
-------------

trident allows you to integrate your existing projects via:

* Extend Python path: add an existing project to scope of trident easily
* Hook required functionality into training via
    1. Linking your own functions
    2. Pytorch-Lightning
    3. Explicit method overrides
    4. Incorporating custom mixins


For details on how to incorporate custom models or datamodules into the framework, see :ref:`customization`.

TODO
-------

The below list for now only reflects the most urgent TODOs in immediate scope of the project. Contributions are very welcome!

* Full out-of-the-box for `XTREME benchmark <https://github.com/google-research/xtreme>`_
* Automated docgen via Github actions to readthedocs
* Basic tests

Credits
-------

* This project is was largely inspired by and is based on https://github.com/ashleve/lightning-hydra-template
* A related project is: https://github.com/PyTorchLightning/lightning-transformers

Author
-------

| Name: Fabian David Schmidt
| Mail: fabian@informatik.uni-mannheim.de
| Affiliation: University of Mannheim
